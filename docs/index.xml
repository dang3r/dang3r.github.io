<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Daniel Cardoza</title>
    <link>https://danielcardoza.com/</link>
    <description>Recent content on Daniel Cardoza</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 31 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://danielcardoza.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>aws-manifest</title>
      <link>https://danielcardoza.com/blog/2019-07/aws-manifest/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://danielcardoza.com/blog/2019-07/aws-manifest/</guid>
      <description>TL:DR pip install aws-manifest / https://github.com/dang3r/aws-manifest
I recently needed to retrieve a list of all Amazon Web Services (AWS) services, and actions that could be called on them. I wanted to answer questions such as &amp;ldquo;What are all of the api actions that can be done for ec2?&amp;ldquo;.
There is no API for doing so from AWS, so I started looking for other options. Some projects scrape the AWS documentation and extract the data from the markup. This is how projects like https://iam.cloudonaut.io/ work. You can see the code they use for scraping and parsing in their github repo here.</description>
    </item>
    
    <item>
      <title>Stratocumulus.cloud</title>
      <link>https://danielcardoza.com/blog/2019-06/stratocumulus.cloud/</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://danielcardoza.com/blog/2019-06/stratocumulus.cloud/</guid>
      <description>TL:DR https://stratocumulus.cloud
Top level domains (TLD) Every so often, I encounter a new top level domain (TLD). TLDs are at the topmost level of domains. A few of the most common are .com, .org, .net and if you are a trendy startup, .io. You can retrieve a full list of TLDs on wikipedia.
Below are some of my favourite TLDs with various subdomains.
 .ninja  coding.ninja sneaky.ninja food.ninja  .rocks  exercise.rocks igneous.rocks ilove.rocks  .life  thegood.life theeasy.life simple.life  .news  fake.news real.news thegood.news   When a new TLD comes out, most of the interesting domains are snatched up quite quickly.</description>
    </item>
    
    <item>
      <title>Clickupy - API &#43; FUSE for Clickup</title>
      <link>https://danielcardoza.com/blog/2019-05/clickupy-api-fuse-for-clickup/</link>
      <pubDate>Sun, 26 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://danielcardoza.com/blog/2019-05/clickupy-api-fuse-for-clickup/</guid>
      <description>TL:DR pip install clickupy / https://github.com/dang3r/clickupy
Over the past few years, I have used many different project management tools. JIRA has been a consistent one, and the tool with the most name recognition. Trello is a very lightweight alternative, and the the one I personally use. Redbooth is another big player and I&amp;rsquo;ve used it heavily (especially while working there!). Wrike and Asana are a few others I&amp;rsquo;ve encountered every now and then.
It was surprising to learn about Clickup, which I have started to use at work. Largely similar to the other solutions provided, I find it strikes a nice balance between being lightweight and powerful.</description>
    </item>
    
    <item>
      <title>Wikireducer : Following Wikipedia links</title>
      <link>https://danielcardoza.com/blog/2018-01/wikireducer-following-wikipedia-links/</link>
      <pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://danielcardoza.com/blog/2018-01/wikireducer-following-wikipedia-links/</guid>
      <description>Overview I came back from the gym last week and a few of my friend&amp;rsquo;s made the following claim:
 Following the first link of a Wikipedia article always leads to philosophy.
 It was a very strong claim, but one that can be verified easily.
Solution My previous scraping projects and linksearchers have been exclusively in Python. I love Python, but I have been exposed to Ruby over the past months and decided this project would be a good fit. 5 months ago I made Seguridad to learn how to make a Ruby gem and increase my exposure to the language.</description>
    </item>
    
    <item>
      <title>Jenophone : SMS forwarder using Twilio</title>
      <link>https://danielcardoza.com/blog/2018-01/jenophone-sms-forwarder-using-twilio/</link>
      <pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://danielcardoza.com/blog/2018-01/jenophone-sms-forwarder-using-twilio/</guid>
      <description>Overview When I moved to California after graduation, it became difficult to easily communicate with my girlfriend in Canada because of telecom issues.
The crux of the problem was the following:
 My phone plan has unlimited data and unlimited texting to US, Canada phone numbers. My girlfriend&amp;rsquo;s phone plan had limited data (thank you Canadian telecoms) and only had unlimited texting in Canada.  I wanted to make communication easier between us, so I decided to make my own SMS forwarding service using Twilio.
Solution My girlfriend&amp;rsquo;s name is Jennifer, hence Jenophone.
Twilio is a company that provides programmable sms, voice and other communication APIs.</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>https://danielcardoza.com/about/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://danielcardoza.com/about/</guid>
      <description>I am a full stack software engineering currently residing in Palo Alto. Currently working at Whiterabbit.ai.</description>
    </item>
    
    <item>
      <title>Cerca : Search a website&#39;s links</title>
      <link>https://danielcardoza.com/blog/2017-11/cerca-search-a-websites-links/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://danielcardoza.com/blog/2017-11/cerca-search-a-websites-links/</guid>
      <description>Overview I recently had a problem where I wanted to verify that a website:
 did not link to a given website on a blacklist did not return non 2XX status codes for a given route  Although there are many utilities you can probably piece together to solve this problem, I decided to make my own utility Cerca (which is Catalan for search).
Problem Given a website URL, search the website for all external and internal links. Follow all internal links while not revisiting past links.
Solution I decided to use the following approach:
 Initialize a queue with the website&amp;rsquo;s root (eg.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://danielcardoza.com/blog/2017-11/introduction/</link>
      <pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://danielcardoza.com/blog/2017-11/introduction/</guid>
      <description>Introduction Welcome to my blog!
This site was created and powered by the following:
 Hugo for static site generation Manis for the website theme Github Pages for hosting  Through this blog, I hope to write interesting posts about projects I am working on, self development and life in general.</description>
    </item>
    
  </channel>
</rss>